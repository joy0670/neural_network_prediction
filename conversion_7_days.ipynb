{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7958f6-318e-4855-baf7-7c39637fca61",
   "metadata": {},
   "source": [
    "# Prediction of conversion within 7 days with neural network model\n",
    "\n",
    "## Part 0: Data initial load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb39ffbe-6905-44d4-8ef8-165b596a3e48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize BigQuery Client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Query to fetch data \n",
    "query = \"\"\"\n",
    "SELECT \n",
    "  event_date,\n",
    "  event_timestamp,\n",
    "  user_pseudo_id,\n",
    "  event_name,\n",
    "  \n",
    "  ecommerce.total_item_quantity AS total_item_quantity,\n",
    "  ecommerce.purchase_revenue_in_usd AS purchase_revenue_in_usd,\n",
    "  ecommerce.purchase_revenue AS purchase_revenue,\n",
    "  ecommerce.unique_items AS unique_items,\n",
    "  ecommerce.transaction_id AS transaction_id,\n",
    "\n",
    "  device.category AS device_category,\n",
    "  device.mobile_brand_name AS device_brand,\n",
    "  device.mobile_model_name AS device_model,\n",
    "  device.operating_system AS operating_system,\n",
    "  device.language AS device_language,\n",
    "\n",
    "  geo.country AS country,\n",
    "  geo.city AS city,\n",
    "\n",
    "  -- Flatten nested 'traffic_source' fields\n",
    "  traffic_source.medium AS traffic_medium,\n",
    "  traffic_source.source AS traffic_source\n",
    "\n",
    "FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_202011*`\n",
    "WHERE event_date BETWEEN '20201101' AND '20201131'\n",
    "\"\"\"\n",
    "query_job = client.query(query)\n",
    "\n",
    "# Load data into a pandas DataFrame\n",
    "df = query_job.to_dataframe()\n",
    "\n",
    "# Convert event_date to datetime\n",
    "df['event_date'] = pd.to_datetime(df['event_date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8a5d4-2ebf-47db-bafa-e5d60414274a",
   "metadata": {},
   "source": [
    "## Part 1.1: Feature Engineering: Create derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12a3d8e-6eb3-4ce5-a3c6-5a948b1968c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_conversion_features(df):\n",
    "    \n",
    "    # Identify conversion\n",
    "    df['is_conversion'] = df['purchase_revenue'] > 0   \n",
    "\n",
    "    # Sort the DataFrame by user_pseudo_id and event_date\n",
    "    df = df.sort_values(by=['user_pseudo_id', 'event_date']).reset_index(drop=True)\n",
    "\n",
    "    # Create a cumulative conversion flag\n",
    "    # Initialize 'last_conversion_date' with NaT for non-conversions and event_date for conversions\n",
    "    df['last_conversion_date'] = pd.NaT\n",
    "    df.loc[df['is_conversion'], 'last_conversion_date'] = df.loc[df['is_conversion'], 'event_date']\n",
    "\n",
    "    # Forward-fill the 'last_conversion_date' for each user\n",
    "    df['last_conversion_date'] = df.groupby('user_pseudo_id')['last_conversion_date'].ffill()\n",
    "\n",
    "    # Step 3: Calculate whether the event is within 7 days of the last conversion\n",
    "    # Ensure 'last_conversion_date' and 'event_date' are in datetime format\n",
    "    df['last_conversion_date'] = pd.to_datetime(df['last_conversion_date'])\n",
    "    df['event_date'] = pd.to_datetime(df['event_date'])\n",
    "\n",
    "    # Calculate the difference in days between 'event_date' and 'last_conversion_date'\n",
    "    df['days_since_conversion'] = (df['event_date'] - df['last_conversion_date']).dt.days\n",
    "\n",
    "    # Identify events within 7 days of the last conversion\n",
    "    df['conversion_within_7_days'] = (\n",
    "        (df['last_conversion_date'].notna()) &  # Ensure a conversion exists\n",
    "        (df['days_since_conversion'] >= 0) &   # Ensure non-negative window\n",
    "        (df['days_since_conversion'] <= 7)     # Within 7 days\n",
    "    )\n",
    "\n",
    "    # Clean up temporary columns\n",
    "    df.drop(columns=['last_conversion_date', 'days_since_conversion'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = add_conversion_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716a268-ca1b-4603-aa52-a83678b318f5",
   "metadata": {},
   "source": [
    "# Part 1.2: Event-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205ab177-d637-4c1b-937d-208dbf8b5e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_event_features(df):    \n",
    "    # One-hot encode event_name\n",
    "    event_name_dummies = pd.get_dummies(df['event_name'], prefix='event')\n",
    "\n",
    "    # Add device_category and traffic_medium as categorical features\n",
    "    df['device_category'] = df['device_category'].astype('category')\n",
    "    df['traffic_medium'] = df['traffic_medium'].astype('category')\n",
    "    \n",
    "    return df,  event_name_dummies\n",
    "\n",
    "df,  event_name_dummies = add_event_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fa2192-25e2-4845-9903-5ec77d1a4d91",
   "metadata": {},
   "source": [
    "# Part 1.3: Session based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f28d16b-b226-457d-8a51-cbfa11825afc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df:\n",
      "['event_date', 'event_timestamp', 'user_pseudo_id', 'event_name', 'total_item_quantity', 'purchase_revenue_in_usd', 'purchase_revenue', 'unique_items', 'transaction_id', 'device_category', 'device_brand', 'device_model', 'operating_system', 'device_language', 'country', 'city', 'traffic_medium', 'traffic_source', 'is_conversion', 'conversion_within_7_days', 'session_id']\n",
      "Columns in session_features:\n",
      "['user_pseudo_id', 'session_id', 'session_duration', 'session_event_count']\n"
     ]
    }
   ],
   "source": [
    "def add_session_features(df):    \n",
    " \n",
    "    # Define session ID using event_timestamp (30-minute of inactivity)\n",
    "    df['event_timestamp'] = pd.to_datetime(df['event_timestamp'], unit='ms')\n",
    "    df = df.sort_values(['user_pseudo_id', 'event_timestamp'])\n",
    "\n",
    "    # Identify session breaks (30 minutes of inactivity), All events within a session (no 30-minute gap) share the same session ID.\n",
    "    df['session_id'] = (df.groupby('user_pseudo_id')['event_timestamp']\n",
    "                          .diff()\n",
    "                          .gt(pd.Timedelta(minutes=30))\n",
    "                          .cumsum())\n",
    "\n",
    "    # Combined session-based feature aggregation\n",
    "    session_features = (\n",
    "        df.groupby(['user_pseudo_id', 'session_id'])\n",
    "        .agg(\n",
    "            session_duration=('event_timestamp', lambda x: (x.max() - x.min()).total_seconds() if len(x) > 1 else 0),\n",
    "            session_event_count=('event_name', 'count')\n",
    "        )\n",
    "        .reset_index()  # Reset index to preserve session-level details\n",
    "    )\n",
    "\n",
    "    # Aggregate session-level data into user-level features\n",
    "    user_session_features = (\n",
    "        session_features.groupby('user_pseudo_id')\n",
    "        .agg(\n",
    "            total_sessions=('session_id', 'count'),\n",
    "            average_session_duration=('session_duration', 'mean'),\n",
    "            total_session_events=('session_event_count', 'sum'),\n",
    "            average_session_events=('session_event_count', 'mean')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Debugging: Confirm session_id exists in df\n",
    "    if 'session_id' not in df.columns:\n",
    "        raise ValueError(\"session_id was not added to df.\")\n",
    "\n",
    "    return df, session_features, user_session_features\n",
    "\n",
    "# Apply the add_session_features function and reassign df\n",
    "df, session_features, user_session_features = add_session_features(df)\n",
    "\n",
    "print(\"Columns in df:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"Columns in session_features:\")\n",
    "print(session_features.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74911102-a7cc-497c-b6f0-237fb7367980",
   "metadata": {},
   "source": [
    "# Part 1-4: Aggregated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cfb90b8-53b6-4b1b-a131-8da486007904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df:\n",
      "['event_date', 'event_timestamp', 'user_pseudo_id', 'event_name', 'total_item_quantity', 'purchase_revenue_in_usd', 'purchase_revenue', 'unique_items', 'transaction_id', 'device_category', 'device_brand', 'device_model', 'operating_system', 'device_language', 'country', 'city', 'traffic_medium', 'traffic_source', 'is_conversion', 'conversion_within_7_days', 'session_id']\n",
      "Columns in session_features:\n",
      "['user_pseudo_id', 'session_id', 'session_duration', 'session_event_count']\n"
     ]
    }
   ],
   "source": [
    "def add_behavioral_features(df):\n",
    "    # Total and average revenue per user\n",
    "    user_aggregates = df.groupby('user_pseudo_id').agg(\n",
    "        total_revenue=('purchase_revenue', 'sum'),\n",
    "        average_revenue=('purchase_revenue', 'mean'),\n",
    "        total_events=('event_name', 'count'),\n",
    "        unique_event_types=('event_name', 'nunique'),\n",
    "        active_days=('event_date', 'nunique')\n",
    "    ).reset_index()\n",
    "    \n",
    "    return df, user_aggregates\n",
    "\n",
    "df, user_aggregates = add_behavioral_features(df)\n",
    "\n",
    "print(\"Columns in df:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"Columns in session_features:\")\n",
    "print(session_features.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01751926-58e7-4f52-aba9-927ab790ab06",
   "metadata": {},
   "source": [
    "# Part 1-5: Behavioral features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d5c607e-c3a9-44fe-8546-3b7a76b51e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in session_features:\n",
      "['user_pseudo_id', 'session_id', 'session_duration', 'session_event_count']\n"
     ]
    }
   ],
   "source": [
    "def add_behavioral_features(df, session_features):\n",
    "    # Create a copy of session_features to avoid in-place modification\n",
    "    session_features_copy = session_features.copy()\n",
    "    \n",
    "    # Abandoned cart: Users who added items to the cart but didn’t purchase\n",
    "    df['abandoned_cart'] = ((df['event_name'] == 'add_to_cart') &\n",
    "                            ~(df['user_pseudo_id'].isin(df.loc[df['event_name'] == 'purchase', 'user_pseudo_id']))).astype(int)\n",
    "    \n",
    "    # Bounce rate: Sessions with only one event\n",
    "    session_features_copy['is_bounce'] = (session_features_copy['session_event_count'] == 1).astype(int)\n",
    "    bounce_rate = session_features_copy.groupby('user_pseudo_id').agg(\n",
    "        bounce_rate=('is_bounce', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Merge bounce rate into the main DataFrame\n",
    "    df = df.merge(bounce_rate, on='user_pseudo_id', how='left')\n",
    "\n",
    "    # Return both original and modified session_features\n",
    "    return df, bounce_rate, session_features, session_features_copy\n",
    "\n",
    "df, bounce_rate, session_features, session_features_copy = add_behavioral_features(df, session_features)\n",
    "\n",
    "print(\"Columns in session_features:\")\n",
    "print(session_features.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438b2e9-1d8e-4d53-a994-416b8d5fd2eb",
   "metadata": {},
   "source": [
    "# Part 1-6: Temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36405852-214c-4747-a2cb-56a59c69b345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_temporal_features(df):\n",
    "    # Calculate days since first event for each user\n",
    "    df['days_since_first_event'] = (df['event_date'] - df.groupby('user_pseudo_id')['event_date'].transform('min')).dt.days\n",
    "\n",
    "    # Days between consecutive events\n",
    "    df['days_between_events'] = df.groupby('user_pseudo_id')['event_date'].diff().dt.days\n",
    "    df['days_between_events'] = df['days_between_events'].fillna(0)  # Fill NaN for first event\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = add_temporal_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a96f89-e3a3-4228-82d6-d3ad6df41781",
   "metadata": {},
   "source": [
    "# Part 1-7: Feature integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f84734b-2d61-4aa6-83ba-c93997b84374",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['event_date', 'event_timestamp', 'user_pseudo_id', 'event_name', 'total_item_quantity', 'purchase_revenue_in_usd', 'purchase_revenue', 'unique_items', 'transaction_id', 'device_category', 'device_brand', 'device_model', 'operating_system', 'device_language', 'country', 'city', 'traffic_medium', 'traffic_source', 'is_conversion', 'conversion_within_7_days', 'session_id', 'abandoned_cart', 'bounce_rate', 'days_since_first_event', 'days_between_events']\n"
     ]
    }
   ],
   "source": [
    "print (df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e86b93af-03f2-429c-bb37-8842c97ad463",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['event_date', 'event_timestamp', 'user_pseudo_id', 'event_name', 'total_item_quantity', 'purchase_revenue_in_usd', 'purchase_revenue', 'unique_items', 'transaction_id', 'device_category', 'device_brand', 'device_model', 'operating_system', 'device_language', 'country', 'city', 'traffic_medium', 'traffic_source', 'is_conversion', 'conversion_within_7_days', 'session_id', 'abandoned_cart', 'bounce_rate', 'days_since_first_event', 'days_between_events', 'total_revenue', 'average_revenue', 'total_events', 'unique_event_types', 'active_days', 'total_sessions', 'average_session_duration', 'total_session_events', 'average_session_events']\n"
     ]
    }
   ],
   "source": [
    "def feature_integration(df):\n",
    "    df = df.merge(user_aggregates, on='user_pseudo_id', how='left')\n",
    "    df = df.merge(user_session_features, on='user_pseudo_id', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "df = feature_integration(df)\n",
    "print (df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f21842-2ef7-42c3-96a6-293d09ea045a",
   "metadata": {},
   "source": [
    "# Part 1-8: A Advanced Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "add3da91-67a0-4672-afcc-8413c6421ef6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def advanced_aggregates(df):\n",
    "    # Conversion rate (conversions/total events per user)\n",
    "    df['conversion_rate'] = df['is_conversion'] / df['total_events']\n",
    "\n",
    "    # Combine all engineered features into the DataFrame\n",
    "    df = pd.concat([df, event_name_dummies], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = advanced_aggregates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1ba09a0-baa3-4316-a9b1-81bdb8fff001",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['event_date', 'event_timestamp', 'user_pseudo_id', 'event_name', 'total_item_quantity', 'purchase_revenue_in_usd', 'purchase_revenue', 'unique_items', 'transaction_id', 'device_category', 'device_brand', 'device_model', 'operating_system', 'device_language', 'country', 'city', 'traffic_medium', 'traffic_source', 'is_conversion', 'conversion_within_7_days', 'session_id', 'abandoned_cart', 'bounce_rate', 'days_since_first_event', 'days_between_events', 'total_revenue', 'average_revenue', 'total_events', 'unique_event_types', 'active_days', 'total_sessions', 'average_session_duration', 'total_session_events', 'average_session_events', 'conversion_rate', 'event_add_payment_info', 'event_add_shipping_info', 'event_add_to_cart', 'event_begin_checkout', 'event_click', 'event_first_visit', 'event_page_view', 'event_purchase', 'event_scroll', 'event_select_item', 'event_select_promotion', 'event_session_start', 'event_user_engagement', 'event_view_item', 'event_view_item_list', 'event_view_promotion', 'event_view_search_results']\n"
     ]
    }
   ],
   "source": [
    "print (df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909a5a10-2b1e-47d9-add9-ba41e86fdd34",
   "metadata": {},
   "source": [
    "# Part 2: Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f135dc0-24e7-4258-be98-4a0e78143db3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1030898, 856)\n",
      "Validation data shape: (220907, 856)\n",
      "Test data shape: (220907, 856)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['prepared_data.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step 1: Define Target Variable and Features\n",
    "# Target variable: conversion_within_7_days\n",
    "target = 'conversion_within_7_days'\n",
    "\n",
    "# Feature columns: Exclude non-useful columns and target\n",
    "exclude_columns = [\n",
    "    'event_date', 'event_timestamp', 'user_pseudo_id', 'transaction_id', \n",
    "    'conversion_within_7_days', 'purchase_revenue_in_usd', 'purchase_revenue', \n",
    "    'average_revenue', 'total_item_quantity', 'unique_items'\n",
    "]\n",
    "features = [col for col in df.columns if col not in exclude_columns]\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Step 2: Data Splitting\n",
    "# Split the data into training, validation, and test sets (70% train, 15% validation, 15% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y) # stratify=y: Ensures the class distribution of the target variable is preserved in the splits\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp) # Further splits the temporary dataset\n",
    "\n",
    "# Step 3: Data Preprocessing\n",
    "# Define numerical and categorical columns, scaling for numerical columns and encoding for categorical ones\n",
    "numerical_columns = X.select_dtypes(include=['int64', 'float64', 'Int64']).columns\n",
    "categorical_columns = X.select_dtypes(include=['category', 'bool', 'object']).columns\n",
    "\n",
    "# Preprocessor for numerical features: Impute missing values and scale\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Replaces missing values with the mean of each column\n",
    "    ('scaler', StandardScaler())                 # Scaling: Standardizes the numerical features to have a mean of 0 and a standard deviation of 1\n",
    "])\n",
    "\n",
    "# Preprocessor for categorical features: Impute and one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fills missing values with the most frequent category\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode, Converts categorical values into binary indicator variables\n",
    "])\n",
    "\n",
    "# Combine preprocessors in a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 4: Final Pipeline\n",
    "# Add preprocessor to a full pipeline\n",
    "modeling_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)  # Preprocessing step (scaling, encoding, etc.)\n",
    "])\n",
    "\n",
    "# Step 5: Fit Preprocessing Pipeline\n",
    "# Fit the preprocessing pipeline only on training data\n",
    "X_train_prepared = modeling_pipeline.fit_transform(X_train)\n",
    "\n",
    "# Apply the preprocessing pipeline to validation and test data\n",
    "X_val_prepared = modeling_pipeline.transform(X_val)\n",
    "X_test_prepared = modeling_pipeline.transform(X_test)\n",
    "\n",
    "# Step 6: Inspect Prepared Data\n",
    "# Check the shape of the transformed data\n",
    "print(f\"Training data shape: {X_train_prepared.shape}\")\n",
    "print(f\"Validation data shape: {X_val_prepared.shape}\")\n",
    "print(f\"Test data shape: {X_test_prepared.shape}\")\n",
    "\n",
    "# Save prepared data for modeling\n",
    "import joblib\n",
    "joblib.dump((X_train_prepared, y_train, X_val_prepared, y_val, X_test_prepared, y_test), 'prepared_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd8c09-c258-4177-832f-89ddbb866151",
   "metadata": {},
   "source": [
    "# Part 3: Neural network\n",
    "- this is a feed forward neural network, data flows in one direction\n",
    "- Why This model is chosen\n",
    "- Tabular Data: Feedforward neural networks are commonly used for structured datasets, especially after preprocessing and feature engineering.\n",
    "- Binary Classification: The sigmoid activation in the output layer ensures the model produces probabilities for the two classes.\n",
    "- Overfitting Prevention: Dropout layers and early stopping help mitigate overfitting.Why This Architecture Was Chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82fe531a-63bf-4887-91a0-32f15e158d41",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 12:32:12.228644: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-20 12:32:16.551354: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-01-20 12:32:16.551795: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-01-20 12:32:16.551818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2025-01-20 12:32:19.588924: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-01-20 12:32:19.590461: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-01-20 12:32:19.590592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (instance-20250107-233026): /proc/driver/nvidia/version does not exist\n",
      "2025-01-20 12:32:19.592274: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                54848     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,961\n",
      "Trainable params: 56,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1. import libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 2. neural network architecture\n",
    "\n",
    "# Build the Neural Network Model\n",
    "nn_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_prepared.shape[1],)),  # Input layer with 64 neurons\n",
    "    Dropout(0.2),  # Dropout for regularization, randomly deactivates 20% of the neurons during training to prevent overfitting\n",
    "    Dense(32, activation='relu'),  # Hidden layer with 32 neurons, ReLU rectified linear unit activation, it introduces non-linearity and helps the model learn complex patterns\n",
    "    Dropout(0.2),  # Dropout for regularization\n",
    "    Dense(1, activation='sigmoid')  # Output layer uses sigmoid activation function, suitable for binary classification, outputs a probability between 1 and 0\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "nn_model.compile(\n",
    "    optimizer='adam',  # this dataset has many features, and Adam is efficient and adapts well to the complexities of training neural networks.\n",
    "    loss='binary_crossentropy',  # Binary crossentropy is suitable for binary classification tasks, as it compares the predicted probability to the actual binary label and calculates the loss accordingly.\n",
    "    metrics=['accuracy']  # Metric to monitor during training\n",
    ")\n",
    "\n",
    "# Display the model architecture\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39462fc5-f0b0-442b-b926-03233db4faff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_train_prepared and X_val_prepared are of type <class 'scipy.sparse._csr.csr_matrix'>, which means they are sparse matrices. Conver sparse matrices to dence. TensorFlow/Keras requires dense arrays for training.\n",
    "# Convert X_train_prepared and X_val_prepared to dense NumPy arrays using .toarray():\n",
    "X_train_prepared = X_train_prepared.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59094324-2a22-4889-b864-3da6c5b7f669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_val_prepared = X_val_prepared.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3556828a-0da6-4560-b4ea-964e014cf3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert y_train and y_val from pandas.Series to NumPy arrays\n",
    "y_train = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f3e732b-d92d-4c5f-a62a-91dc8d85e245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_val = y_val.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47abe448-5bda-4bfd-8d04-53ff397b0fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. train neural network\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=5,          # Stop if no improvement after 5 epochs\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best validation loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae6efc-efdd-4bd3-ad96-2dc6c5042910",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the Neural Network\n",
    "history = nn_model.fit(\n",
    "    X_train_prepared, y_train,  # Training data\n",
    "    validation_data=(X_val_prepared, y_val),  # Validation data\n",
    "    epochs=5,  # Maximum number of epochs\n",
    "    batch_size=1,  # Number of samples per batch\n",
    "    callbacks=[early_stopping],  # Early stopping callback\n",
    "    verbose=1  # Print training progress\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce8212-7291-4b2b-bb5b-954a15232783",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. evaluate neural network\n",
    "\n",
    "# Evaluate on Validation Data\n",
    "y_pred_prob_nn = nn_model.predict(X_val_prepared).flatten()  # Predicted probabilities\n",
    "y_pred_nn = (y_pred_prob_nn > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report (Neural Network):\")\n",
    "print(classification_report(y_val, y_pred_nn))\n",
    "\n",
    "# AUC-ROC\n",
    "auc_roc_nn = roc_auc_score(y_val, y_pred_prob_nn)\n",
    "print(\"AUC-ROC (Neural Network):\", auc_roc_nn)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_nn = confusion_matrix(y_val, y_pred_nn)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix_nn, annot=True, fmt='d', cmap='Blues', xticklabels=['No Conversion', 'Conversion'], yticklabels=['No Conversion', 'Conversion'])\n",
    "plt.title('Confusion Matrix: Neural Network')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred_prob_nn)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC-ROC = {auc_roc_nn:.2f}\", color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Reference line for random guess\n",
    "plt.title('ROC Curve: Neural Network')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# 5. hyperparameter optimization for neural networks\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Define the model-building function\n",
    "def build_nn_model(hp):\n",
    "    model = Sequential([\n",
    "        Dense(hp.Int('units_1', min_value=32, max_value=128, step=32), activation='relu', input_shape=(X_train_prepared.shape[1],)),\n",
    "        Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)),\n",
    "        Dense(hp.Int('units_2', min_value=16, max_value=64, step=16), activation='relu'),\n",
    "        Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=hp.Choice('optimizer', ['adam', 'sgd']),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Initialize Keras Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_nn_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # Number of different hyperparameter combinations to try\n",
    "    executions_per_trial=1,  # Number of times to evaluate each combination\n",
    "    directory='my_dir',\n",
    "    project_name='nn_tuning'\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(X_train_prepared, y_train, validation_data=(X_val_prepared, y_val), epochs=50, batch_size=256, callbacks=[early_stopping])\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best Hyperparameters: {best_hps.values}\")\n",
    "\n",
    "# Build and train the best model\n",
    "best_nn_model = tuner.hypermodel.build(best_hps)\n",
    "best_nn_model.fit(X_train_prepared, y_train, validation_data=(X_val_prepared, y_val), epochs=50, batch_size=256, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ca63e-4038-4ae7-b481-9ad5b9acb88a",
   "metadata": {},
   "source": [
    "# Part 4: Neural network evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712a5c4-80f7-4d47-a252-93fb9e2c973d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Generate Predictions\n",
    "# Predict probabilities and binary labels\n",
    "y_pred_prob_nn = nn_model.predict(X_test_prepared).flatten()  # Probabilities\n",
    "y_pred_nn = (y_pred_prob_nn > 0.5).astype(int)  # Binary predictions with a threshold of 0.5\n",
    "\n",
    "# 2. Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report (Neural Network):\")\n",
    "print(classification_report(y_test, y_pred_nn))\n",
    "\n",
    "# 3. AUC-ROC Score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_roc_nn = roc_auc_score(y_test, y_pred_prob_nn)\n",
    "print(f\"AUC-ROC Score (Neural Network): {auc_roc_nn:.2f}\")\n",
    "\n",
    "# 4. Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_matrix_nn = confusion_matrix(y_test, y_pred_nn)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix_nn, annot=True, fmt='d', cmap='Blues', xticklabels=['No Conversion', 'Conversion'], yticklabels=['No Conversion', 'Conversion'])\n",
    "plt.title('Confusion Matrix: Neural Network')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# 5. ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_nn, tpr_nn, thresholds_nn = roc_curve(y_test, y_pred_prob_nn)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_nn, tpr_nn, label=f\"AUC-ROC = {auc_roc_nn:.2f}\", color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.title('ROC Curve: Neural Network')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# 6. Precision-Recall Curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision_nn, recall_nn, thresholds_nn = precision_recall_curve(y_test, y_pred_prob_nn)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_nn, precision_nn, color='blue')\n",
    "plt.title('Precision-Recall Curve: Neural Network')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacd2bfc-1bf9-47ff-8110-04e520a0cd7a",
   "metadata": {},
   "source": [
    "# Part 5: Deployment preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892460c-345b-41b9-a10f-d0ab8598396a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from google.cloud import aiplatform\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Step 1: Save the model and preprocessing pipeline locally\n",
    "print(\"Saving model and preprocessing pipeline...\")\n",
    "nn_model.save('neural_network_model.h5')  # Save the trained neural network model\n",
    "joblib.dump(modeling_pipeline, 'preprocessing_pipeline.pkl')  # Save the preprocessing pipeline\n",
    "print(\"Model and preprocessing pipeline saved successfully.\")\n",
    "\n",
    "# Step 2: Export the model to TensorFlow SavedModel format\n",
    "print(\"Exporting model to TensorFlow SavedModel format...\")\n",
    "model = load_model('neural_network_model.h5')\n",
    "model.save('saved_model_dir')  # Create SavedModel directory\n",
    "print(\"Model exported successfully.\")\n",
    "\n",
    "# Step 3: Upload artifacts to Google Cloud Storage (GCS)\n",
    "print(\"Uploading artifacts to GCS...\")\n",
    "model_gcs_path = 'gs://my-project-2025-447122-eu-notebooks/model/'\n",
    "os.system(f\"gsutil cp -r saved_model_dir {model_gcs_path}\")\n",
    "os.system(f\"gsutil cp preprocessing_pipeline.pkl {model_gcs_path}\")\n",
    "print(\"Artifacts uploaded to GCS successfully.\")\n",
    "\n",
    "# Step 4: Initialize the Vertex AI environment\n",
    "print(\"Initializing Vertex AI environment...\")\n",
    "aiplatform.init(\n",
    "    project='my-project-2025-447122',  # Replace with your GCP project ID\n",
    "    location='europe-west4'            # Replace with your GCP region\n",
    ")\n",
    "\n",
    "# Step 5: Upload the model to Vertex AI\n",
    "print(\"Uploading model to Vertex AI...\")\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name='neural-network-model',  # Display name for the model in Vertex AI\n",
    "    artifact_uri=f\"{model_gcs_path}saved_model_dir\",  # Path to the SavedModel in GCS\n",
    "    serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest',  # TensorFlow serving container\n",
    ")\n",
    "print(f\"Model uploaded successfully: {model.resource_name}\")\n",
    "\n",
    "# Step 6: Deploy the model to an endpoint\n",
    "print(\"Deploying model to an endpoint...\")\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name='nn_model_endpoint',  # Name for the deployed model\n",
    "    machine_type='n1-standard-4',                     # Machine type for hosting\n",
    ")\n",
    "print(f\"Model deployed successfully: {endpoint.resource_name}\")\n",
    "\n",
    "print(\"Deployment preparation completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9908edd9-c5e1-4b56-ad82-e995b4e29f40",
   "metadata": {},
   "source": [
    "# Part 5: Send prediction request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac540b5-eed0-4074-84c1-8a476fb3c9da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize Vertex AI environment\n",
    "aiplatform.init(project=\"my-project-2025-447122\", location=\"europe-west4\")\n",
    "\n",
    "# Define endpoint and pipeline path\n",
    "endpoint = aiplatform.Endpoint(\"projects/190053636941/locations/europe-west4/endpoints/4825398093518209024\")\n",
    "pipeline_path = \"preprocessing_pipeline.pkl\"  # Path to the preprocessing pipeline\n",
    "\n",
    "# Step 1: Fetch and Prepare Data\n",
    "\n",
    "def fetch_and_prepare_data(date):\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "      event_date,\n",
    "      event_timestamp,\n",
    "      user_pseudo_id,\n",
    "      event_name,\n",
    "      ecommerce.total_item_quantity AS total_item_quantity,\n",
    "      ecommerce.purchase_revenue_in_usd AS purchase_revenue_in_usd,\n",
    "      ecommerce.purchase_revenue AS purchase_revenue,\n",
    "      ecommerce.unique_items AS unique_items,\n",
    "      ecommerce.transaction_id AS transaction_id,\n",
    "      device.category AS device_category,\n",
    "      device.mobile_brand_name AS device_brand,\n",
    "      device.mobile_model_name AS device_model,\n",
    "      device.operating_system AS operating_system,\n",
    "      device.language AS device_language,\n",
    "      geo.country AS country,\n",
    "      geo.city AS city,\n",
    "      traffic_source.medium AS traffic_medium,\n",
    "      traffic_source.source AS traffic_source\n",
    "    FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n",
    "    WHERE event_date = '{date}'\n",
    "    \"\"\"\n",
    "\n",
    "    query_job = client.query(query)\n",
    "    df = query_job.to_dataframe()\n",
    "\n",
    "    # Apply feature engineering\n",
    "    df = feature_engineering(df)\n",
    "    return df\n",
    "\n",
    "# Reuse your feature engineering functions\n",
    "def feature_engineering(df):\n",
    "    df = add_conversion_features(df)  # Returns only df\n",
    "    df, event_name_dummies = add_event_features(df)  # Unpack the tuple\n",
    "    df, session_features, user_session_features = add_session_features(df)  # Unpack the tuple \n",
    "    df, bounce_rate, session_features, session_features_copy = add_behavioral_features(df, session_features)\n",
    "    if 'bounce_rate' in df.columns:\n",
    "        print (\"bounce_rate in df\")\n",
    "    else:\n",
    "        print (\"bounce_rate NOT in df\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    df = add_temporal_features(df)  # Returns only df\n",
    "    df = feature_integration(df)  # Ensure all dependencies are properly passed\n",
    "    df = advanced_aggregates(df)  # Returns only df\n",
    "    return df\n",
    "\n",
    "\n",
    "# Step 2: Preprocess the Data\n",
    "\n",
    "def preprocess_data(df, pipeline_path):\n",
    "    \"\"\"\n",
    "    Preprocess the data using the saved preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    print(\"Loading preprocessing pipeline...\")\n",
    "    pipeline = joblib.load(pipeline_path)\n",
    "    print(\"Preprocessing pipeline loaded successfully.\")\n",
    "\n",
    "    print(\"Transforming data...\")\n",
    "    prepared_data = pipeline.transform(df)\n",
    "    print(\"Data transformation completed.\")\n",
    "\n",
    "    return prepared_data\n",
    "\n",
    "# Step 3: Send Prediction Request\n",
    "\n",
    "def send_prediction_request(prepared_data, endpoint):\n",
    "    \"\"\"\n",
    "    Send the prepared data to the Vertex AI endpoint for prediction.\n",
    "    \"\"\"\n",
    "    print(\"Sending data to the endpoint for prediction...\")\n",
    "    instances = prepared_data.tolist()  # Convert to a JSON-ready format\n",
    "    response = endpoint.predict(instances=instances)\n",
    "    print(\"Prediction completed.\")\n",
    "    return response.predictions\n",
    "\n",
    "# Main Function\n",
    "\n",
    "def main():\n",
    "    # Define the prediction date\n",
    "    prediction_date = \"2021-01-31\"\n",
    "\n",
    "    print(f\"Fetching and preparing data for {prediction_date}...\")\n",
    "    raw_data = fetch_and_prepare_data(prediction_date)\n",
    "    print(f\"Raw data loaded with shape: {raw_data.shape}\")\n",
    "\n",
    "    print(\"Preprocessing data...\")\n",
    "    prepared_data = preprocess_data(raw_data, pipeline_path)\n",
    "    print(f\"Prepared data shape: {prepared_data.shape}\")\n",
    "\n",
    "    print(\"Sending prediction request...\")\n",
    "    predictions = send_prediction_request(prepared_data, endpoint)\n",
    "\n",
    "    print(\"Predictions:\")\n",
    "    print(predictions)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a3038-9acf-4289-9951-f456ab7732d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
